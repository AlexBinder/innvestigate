1) AddLayer und AVeragePooling als ReverseLayer (ohne weight copy)

2) (redacted) Refactor ReverseLayer [...]
2.1) updated: ReverseLayer classes for layers without weight kernels.
2.1.0) refactor special ReverseLayer code. Add secondary SpecialREverseLayer class, feeding of alternate rule dictionary.
2.1.1) BatchNormalization: Support for eps / alpha beta / flat type rules.
2.1.3) Add type layers: Support for eps / alpha beta / flat type rules.
2.1.4) !! Nonlinearities and activation layers (handle non-zero-centered fxns. they break things. Exceptins!)
2.1.5) Pooling type layers : Support for eps / alpha beta / flat type rules.

3) allow conditioned rule mappings to depend on layer index, e.g. for selecting layers for FlatRule
4) allow selection class to LRP-decompose based on class index #TODO: write issue: implement fxn go over keras graph and return a list of layers so the indexing of layers is known/correct
5) numerical testing for supported layers:
5.1) Activation
5.2) Add
5.3) [Global]AveragePooling[1-3]D
5.4) [Global]MaxPooling[1-3]D
5.5) BatchNormalization
5.*)
6) numerical testing for rule types on layers with kernels and pooling layers.
6.*)

7) Parameter tuning for LRP and
7.1) resnet50
7.2) inception_v3
7.3) *

9) ipython notebooks showing analyzer setup and influence of parameters

(max) One of the application networks uses Lambda-Layers. Probably needs special treatment.
