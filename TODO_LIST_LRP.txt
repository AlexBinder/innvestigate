- Add and Test fully connected models from LRP Toolbox (data normalized to [-1 1], relu and tanh activations. get code from laptop.)
- ReverseLayer as independent class
    + Merge type layers
    + Batch norm layer
    + nonlinearities and activations. handle non-zero-centered activation fxns. they break things
    + add-layer
- write test showcasing the impact of parameter choices for alpha, beta , eps
-For MNIST, load in PLOS models which operate on [-1 1] data for additional heatmap info
-INDEX-based relevance computation(!)
-class for assigning LRPAlphaBeta21 to conv layers and eps to dense layers
-make FLAT special case of W^2.
-ensure W^2 works with layers without kernels (e.g. pooling)
-parameterized LRP: implement support for rule objects instead of rule classes ?
-numerical testing for models and layers.
-ReverseLayer for MaxPooling and SumPooling

-todo: after release: consider allowing to pass rule instances instead of rule classes, for adding parameters. the current pattern seems over-engineered.

