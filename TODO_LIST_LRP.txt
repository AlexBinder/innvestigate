- Add and Test fully connected models from LRP Toolbox (data normalized to [-1 1], relu and tanh activations. get code from laptop.)
- ReverseLayer as independent class
    + Merge type layers
    + Batch norm layer
    + nonlinearities and activations. handle non-zero-centered activation fxns. they break things
    + add-layer
- write test showcasing the impact of parameter choices for alpha, beta , eps
-Fix alpha-beta-rules to not rely on inputs being positive:
    + currently, positive and negative weights are handled independently.
    + should be: positive and negative preactivations Zpos and Zneg.
    + this covers the case where inputs are assumed to be positive.
    + this includes the bias.
-Fix Zplus rule to not rely on positive inputs:
    + either fix directly
    + in intherit alphaBeta and set alpha1 beta0
-For MNIST, load in PLOS models which operate on [-1 1] data for additional heatmap info
-INDEX-based relevance computation(!)
-class for assigning LRPAlphaBeta21 to conv layers and eps to dense layers
-make FLAT special case of W^2.
-ensure W^2 works with layers without kernels (e.g. pooling)
-parameterized LRP: implement support for rule objects instead of rule classes ?

-todo: after release: consider allowing to pass rule instances instead of rule classes, for adding parameters. the current pattern seems over-engineered.

