

2) (redacted) Refactor ReverseLayer [...]
2.1) Ensure ReverseLayer works for layers without kernels (some LRP methods will make no sense/are the same, others do.)
2.1.1) BatchNormalization
2.1.2) Merge type layers
2.1.3) Add type layers
2.1.4) Nonlinearities and activation layers (handle non-zero-centered fxns. they break things)
2.1.5) Pooling type layers (Flat decomp might be desirable here. Rules in general. Need ReverseLayer subclass for rules?)

3) allow conditioned rule mappings to depend on layer index, e.g. for selecting layers for FlatRule
4) allow selection class to LRP-decompose based on class index #TODO: write issue: implement fxn go over keras graph and return a list of layers so the indexing of layers is known/correct
5) numerical testing for supported layers:
5.1) Activation
5.2) Add
5.3) [Global]AveragePooling[1-3]D
5.4) [Global]MaxPooling[1-3]D
5.5) BatchNormalization
5.*)
6) numerical testing for rule types on layers with kernels and pooling layers.
6.*)
7) ipython notebooks showing analyzer setup and influence of parameters

