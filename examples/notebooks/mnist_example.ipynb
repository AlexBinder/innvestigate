{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how saliency maps are computed by various methods for smaller, not so massive neural networks based on the well-known `MNIST` data set. For a first glimpse onto the general workflow for using the `iNNvestigate` toolbox, or for users without access to beefy GPU or CPU hardware with tons of RAM, this is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import imp\n",
    "import time\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.tests.networks.base\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "import innvestigate.applications\n",
    "import innvestigate.applications.mnist\n",
    "\n",
    "\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "mnistutils = imp.load_source(\"utils_mnist\", \"../utils_mnist.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Then, the MNIST data is loaded in its entirety, formatted wrt to the specifications of the keras backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "channels_first = keras.backend.image_data_format == \"channels_first\"\n",
    "data = mnistutils.fetch_data(channels_first) #returns x_train, y_train, x_test, y_test as numpy.ndarray\n",
    "num_classes = len(np.unique(data[1]))\n",
    "\n",
    "# Test samples for illustrations\n",
    "images = [(data[2][i].copy(), data[3][i]) for i in range(num_classes)]\n",
    "label_to_class_name = [str(i) for i in range(num_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We have prepared an (extendable) dictionary of neural network architectures to play around with, some of which are already pre-trained and some which have not seen any `MNIST` data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         MODELNAME                      INPUT RANGE   EPOCHS   BATCH_SZ   MODEL CREATION KWARGS\n",
    "models = {'mlp_2dense':                  ([-1, 1],     15,      128,       {'dense_units':1024, 'dropout_rate':0.25}),\n",
    "          'mlp_3dense':                  ([-1, 1],     20,      128,       {'dense_units':1024, 'dropout_rate':0.25}),\n",
    "          'cnn_2convb_2dense':           ([-.5, .5],   20,      64,        {}),\n",
    "\n",
    "          # pre-trained model from [https://doi.org/10.1371/journal.pone.0130140 , http://jmlr.org/papers/v17/15-618.html]\n",
    "          'pretrained_plos_long_relu':   ([-1, 1],     0,       0,         {}),\n",
    "          'pretrained_plos_short_relu':  ([-1, 1],     0,       0,         {}),\n",
    "          'pretrained_plos_long_tanh':   ([-1, 1],     0,       0,         {}),\n",
    "          'pretrained_plos_short_tanh':  ([-1, 1],     0,       0,         {}),\n",
    "          }\n",
    "#Adapt and Play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select one of the above models by setting the variable `modelname` as below. The corresponding parameters regarding expected input data range, number of training epochs and optional model definition parameters will be fetched from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack model params by name. Currently, the line below selects an already pretrained network.\n",
    "modelname = 'pretrained_plos_long_relu'\n",
    "activation_type = 'relu'\n",
    "input_range, epochs, batch_size, kwargs = models[modelname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, preprocess the model wrt to the model's requirements, build the model and optionally train it for `epochs` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02966736011942753\n",
      "Test accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "data_preprocessed = (mnistutils.preprocess(data[0], input_range), data[1],\n",
    "                     mnistutils.preprocess(data[2], input_range), data[3])\n",
    "\n",
    "\n",
    "# Create & (optionally) train model\n",
    "model, modelp = mnistutils.create_model(channels_first, modelname, **kwargs)\n",
    "mnistutils.train_model(modelp, data_preprocessed, batch_size=batch_size, epochs=epochs)\n",
    "model.set_weights(modelp.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up a list of analysis methods by preparing tuples containing the methods' string identifiers used by `innvestigate.analyzer.create_analyzer(...)`, some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render. Analyzers can be deactivated by simply commenting the corresponding lines, or added by creating a new tuple as below.\n",
    "\n",
    "For a full list of methods refer to the dictionary `investigate.analyzer.analyzers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine analysis methods and properties\n",
    "methods = [\n",
    "    # NAME                    OPT.PARAMS               POSTPROC FXN                TITLE\n",
    "\n",
    "    # Show input\n",
    "    (\"input\",                 {},                       mnistutils.image,          \"Input\"),\n",
    "\n",
    "    # Function\n",
    "    (\"gradient\",              {},                       mnistutils.graymap,        \"Gradient\"),\n",
    "    (\"smoothgrad\",            {\"noise_scale\": 50},      mnistutils.graymap,        \"SmoothGrad\"),\n",
    "    (\"integrated_gradients\",  {},                       mnistutils.graymap,        \"Integrated Gradients\"),\n",
    "\n",
    "    # Signal\n",
    "    (\"deconvnet\",             {},                       mnistutils.bk_proj,        \"Deconvnet\"),\n",
    "    (\"guided_backprop\",       {},                       mnistutils.bk_proj,        \"Guided Backprop\",),\n",
    "    (\"pattern.net\",           {},                       mnistutils.bk_proj,        \"PatternNet\"),\n",
    "\n",
    "    # Interaction\n",
    "    (\"lrp.z_baseline\",        {},                       mnistutils.heatmap,         \"Gradient*Input\"),\n",
    "    (\"lrp.z\",                 {},                       mnistutils.heatmap,         \"LRP-Z\"),\n",
    "    (\"lrp.epsilon\",           {\"epsilon\": 1},           mnistutils.heatmap,         \"LRP-Epsilon\"),\n",
    "    (\"lrp.composite_a\",       {},                       mnistutils.heatmap,         \"LRP-CompositeA\"),\n",
    "    #(\"lrp.composite_b\",       {\"epsilon\": 1},           mnistutils.heatmap,         \"LRP-CompositeB\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop below will now instantiate the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above and compute the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analyzer instances: \n",
      "Input Gradient SmoothGrad Integrated Gradients Deconvnet Guided Backprop PatternNet Epoch 1/1\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 4.0000 - broadcast_1_loss: 1.0000 - broadcast_2_loss: 1.0000 - broadcast_3_loss: 1.0000 - broadcast_4_loss: 1.0000\n",
      "Gradient*Input LRP-Z LRP-Epsilon LRP-CompositeA \n",
      "\n",
      "Image 0: Input (0.050s) Gradient (0.102s) SmoothGrad (0.270s) Integrated Gradients (0.323s) Deconvnet (0.258s) Guided Backprop (0.129s) PatternNet (1.766s) Gradient*Input (0.146s) LRP-Z (0.474s) LRP-Epsilon (0.413s) LRP-CompositeA (0.422s) \n",
      "Image 1: Input (0.002s) Gradient (0.004s) SmoothGrad (0.006s) Integrated Gradients (0.006s) Deconvnet (0.004s) Guided Backprop (0.005s) PatternNet (0.009s) Gradient*Input (0.005s) LRP-Z (0.007s) LRP-Epsilon (0.012s) LRP-CompositeA (0.008s) \n",
      "Image 2: Input (0.002s) Gradient (0.009s) SmoothGrad (0.010s) Integrated Gradients (0.013s) Deconvnet (0.007s) Guided Backprop (0.005s) PatternNet (0.005s) Gradient*Input (0.004s) LRP-Z (0.004s) LRP-Epsilon (0.005s) LRP-CompositeA (0.005s) \n",
      "Image 3: Input (0.003s) Gradient (0.004s) SmoothGrad (0.007s) Integrated Gradients (0.004s) Deconvnet (0.004s) Guided Backprop (0.004s) PatternNet (0.004s) Gradient*Input (0.003s) LRP-Z (0.003s) LRP-Epsilon (0.003s) LRP-CompositeA (0.004s) \n",
      "Image 4: Input (0.001s) Gradient (0.003s) SmoothGrad (0.004s) Integrated Gradients (0.004s) Deconvnet (0.003s) Guided Backprop (0.003s) PatternNet (0.004s) Gradient*Input (0.005s) LRP-Z (0.003s) LRP-Epsilon (0.003s) LRP-CompositeA (0.004s) \n",
      "Image 5: Input (0.003s) Gradient (0.005s) SmoothGrad (0.010s) Integrated Gradients (0.005s) Deconvnet (0.004s) Guided Backprop (0.003s) PatternNet (0.004s) Gradient*Input (0.003s) LRP-Z (0.003s) LRP-Epsilon (0.003s) LRP-CompositeA (0.004s) \n",
      "Image 6: Input (0.002s) Gradient (0.003s) SmoothGrad (0.003s) Integrated Gradients (0.003s) Deconvnet (0.003s) Guided Backprop (0.004s) PatternNet (0.004s) Gradient*Input (0.003s) LRP-Z (0.003s) LRP-Epsilon (0.003s) LRP-CompositeA (0.003s) \n",
      "Image 7: Input (0.002s) Gradient (0.048s) SmoothGrad (0.011s) Integrated Gradients (0.008s) Deconvnet (0.003s) Guided Backprop (0.003s) PatternNet (0.004s) Gradient*Input (0.003s) LRP-Z (0.004s) LRP-Epsilon (0.004s) LRP-CompositeA (0.004s) \n",
      "Image 8: Input (0.009s) Gradient (0.003s) SmoothGrad (0.010s) Integrated Gradients (0.008s) Deconvnet (0.002s) Guided Backprop (0.003s) PatternNet (0.004s) Gradient*Input (0.005s) LRP-Z (0.005s) LRP-Epsilon (0.004s) LRP-CompositeA (0.003s) \n",
      "Image 9: Input (0.002s) Gradient (0.003s) SmoothGrad (0.004s) Integrated Gradients (0.005s) Deconvnet (0.003s) Guided Backprop (0.003s) PatternNet (0.003s) Gradient*Input (0.003s) LRP-Z (0.003s) LRP-Epsilon (0.003s) LRP-CompositeA (0.003s) \n"
     ]
    }
   ],
   "source": [
    "# Create analyzers.\n",
    "analyzers = []\n",
    "print('Creating analyzer instances: ', flush=True)\n",
    "for method in methods:\n",
    "    print(method[3], end=' ', flush=True)\n",
    "    analyzer = innvestigate.create_analyzer(method[0],   # analysis method identifier\n",
    "                                            model,       # model without softmax output\n",
    "                                            **method[1]) # optional analysis parameters\n",
    "    # some analyzers require additional training. For those\n",
    "    analyzer.fit(data_preprocessed[0],\n",
    "                 pattern_type=activation_type,\n",
    "                 batch_size=256, verbose=1)\n",
    "    analyzers.append(analyzer)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Apply analyzers to trained model.\n",
    "analysis = np.zeros([len(images), len(analyzers), 28, 28, 3])\n",
    "text = []\n",
    "for i, (image, y) in enumerate(images):\n",
    "    print('Image {}: '.format(i), end='', flush=True)\n",
    "    image = image[None, :, :, :]\n",
    "    # Predict label.\n",
    "    x = mnistutils.preprocess(image, input_range)\n",
    "    presm = model.predict_on_batch(x)[0] #forward pass without softmax\n",
    "    prob = modelp.predict_on_batch(x)[0] #forward pass with softmax\n",
    "    y_hat = prob.argmax()\n",
    "    \n",
    "    # Save prediction info:\n",
    "    text.append((\"%s\" %label_to_class_name[y],    # ground truth label\n",
    "                 \"%.2f\" %presm.max(),             # pre-softmax logits\n",
    "                 \"%.2f\" % prob.max(),             # probabilistic softmax output  \n",
    "                 \"%s\" %label_to_class_name[y_hat] # predicted label\n",
    "                ))\n",
    "    \n",
    "    for aidx, analyzer in enumerate(analyzers):\n",
    "        # Measure execution time\n",
    "        t_start = time.time()\n",
    "        print('{} '.format(''.join(methods[aidx][-1])), end='', flush=True)\n",
    "        \n",
    "        is_input_analyzer = methods[aidx][0] == \"input\"\n",
    "        # Analyze.\n",
    "        a = analyzer.analyze(image if is_input_analyzer else x)\n",
    "        \n",
    "        t_elapsed = time.time() - t_start\n",
    "        print('({:.3f}s) '.format(t_elapsed), end='', flush=True)\n",
    "        \n",
    "        # Postprocess.\n",
    "        if not is_input_analyzer:\n",
    "            a = mnistutils.postprocess(a)\n",
    "        a = methods[aidx][2](a)\n",
    "        analysis[i, aidx] = a[0]\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the analysis times for `Image 0` is substantially longer for all methods, compared to the follow-ups. Each analyzer manages a computational graph, it attaches to the analyzed model. Upon first execution, the analyzer needs to build that graph, which can take some time. Each consecutive analysis on the same model though is almost instant.\n",
    "\n",
    "Next, we visualize the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the analysis.\n",
    "grid = [[analysis[i, j] for j in range(analysis.shape[1])]\n",
    "        for i in range(analysis.shape[0])]\n",
    "row_labels = text\n",
    "col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "eutils.plot_image_grid(grid, row_labels, col_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This figure shows the analysis regarding the *actually predicted* class as computed by the selected analyzers. Each column shows the visualized results for different analyzers and each row shows the analyses wrt to one input sample. To the left of each row, the ground truth label `label` and the predicted label `pred` are show. To the right, the model's probabilistic (softmax) output is shown as `prob` and the logit output just before the terminating softmax layer as `logit`. Note that all analyses have been performed based on the logit output (layer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
