{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenet Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of the proposed in(n)vestigation methods on the ImageNet dataset for a pretrained VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.tests.networks.base\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "imgnetutils = imp.load_source(\"utils_imagenet\", \"../utils_imagenet.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a few (preselected) example images from the ImageNet dataset. In order to load your choice of images from the ImageNet dataset, place them into `innvestigate/examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get some example test set images.\n",
    "images, label_to_class_name = eutils.get_imagenet_data()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained VGG16 net with ReLUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter\n",
    "netname = \"vgg16\"\n",
    "pattern_type = \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "channels_first = keras.backend.image_data_format == \"channels_first\"\n",
    "\n",
    "tmp = getattr(innvestigate.applications.imagenet, netname)\n",
    "net = tmp(load_weights=True, load_patterns=pattern_type)\n",
    "model = keras.models.Model(inputs=net[\"in\"], outputs=net[\"out\"])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "modelp = keras.models.Model(inputs=net[\"in\"], outputs=net[\"sm_out\"])\n",
    "modelp.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use several methods to in(n)vestigate and visualize the learned classifier on the example images.\n",
    "\n",
    "Hereby the analyse method containing name, optional parameter, postprocessing information and a title for the final visualisation is passed into the framework by a n-tuple. The available methods are subdivided into three groups depending on their principal approach: gradient-based, pattern-based and relevance-based investigation methods. For a full list of methods please refer to the script in `innvestigate/innvestigate/analyzer/__init__.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of in(n)vestigation methods we want to apply to the trained model, if applicable specify optional \n",
    "# parameters\n",
    "\n",
    "methods = [\n",
    "    # NAME             POSTPROCESSING     TITLE\n",
    "\n",
    "    # Show input.\n",
    "    (\"input\",                 {},                            imgnetutils.image,   \"Input\"),\n",
    "\n",
    "    # Function\n",
    "    (\"gradient\",              {},                            imgnetutils.graymap, \"Gradient\"),\n",
    "    (\"smoothgrad\",            {\"noise_scale\": 50},           imgnetutils.graymap, \"SmoothGrad\"),\n",
    "    (\"integrated_gradients\",  {},                            imgnetutils.graymap, (\"Integrated\", \"Gradients\")),\n",
    "\n",
    "    # Signal\n",
    "    (\"deconvnet\",             {},                            imgnetutils.bk_proj, \"Deconvnet\"),\n",
    "    (\"guided_backprop\",       {},                            imgnetutils.bk_proj, (\"Guided\", \"Backprop\"),),\n",
    "    (\"pattern.net\",           {\"patterns\": net[\"patterns\"]}, imgnetutils.bk_proj, \"PatterNet\"),\n",
    "\n",
    "    # Interaction\n",
    "    (\"pattern.attribution\",   {\"patterns\": net[\"patterns\"]}, imgnetutils.heatmap, \"Pattern\", \"Attribution\"),\n",
    "    (\"lrp.epsilon\",           {},                            imgnetutils.heatmap, \"LRP Epsilon\"),\n",
    "    (\"lrp.composite_a\",       {},                            imgnetutils.heatmap, \"LRP CompositeA\"),\n",
    "    (\"lrp.composite_b\",       {},                            imgnetutils.heatmap, \"LRP CompositeB\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create analyzer for \n",
      "Input \n",
      "Gradient \n",
      "SmoothGrad \n",
      "IntegratedGradients \n",
      "Deconvnet \n",
      "GuidedBackprop \n",
      "PatterNet \n",
      "Pattern \n",
      "LRP Epsilon \n",
      "LRP CompositeA \n",
      "LRP CompositeB \n",
      "Input (0.0131s) Gradient (1.6876s) SmoothGrad (127.4631s) IntegratedGradients (122.1746s) Deconvnet (3.7528s) GuidedBackprop (2.8991s) PatterNet (10.7435s) Attribution (11.9325s) LRP Epsilon (3.2101s) LRP CompositeA (6.3730s) LRP CompositeB (11.2736s) Input (0.0015s) Gradient (2.5688s) SmoothGrad (142.8500s) IntegratedGradients (134.9370s) Deconvnet (2.2306s) GuidedBackprop (2.0655s) PatterNet (2.1790s) Attribution (2.0912s) LRP Epsilon (2.0623s) LRP CompositeA (4.9453s) LRP CompositeB (9.7332s) Input (0.0058s) Gradient (2.2173s) SmoothGrad (113.9932s) IntegratedGradients (109.9991s) Deconvnet (1.8045s) GuidedBackprop (1.7679s) PatterNet (1.8361s) Attribution (1.8002s) LRP Epsilon (1.8081s) LRP CompositeA (4.3137s) LRP CompositeB (7.6026s) Input (0.0012s) Gradient (1.7926s) SmoothGrad (111.3442s) IntegratedGradients (109.2471s) Deconvnet (1.7484s) GuidedBackprop (1.7498s) PatterNet (1.7748s) Attribution (1.7822s) LRP Epsilon (1.7731s) LRP CompositeA (4.2379s) LRP CompositeB (7.4290s) Input (0.0012s) Gradient (1.7748s) SmoothGrad (109.2198s) IntegratedGradients (108.4567s) Deconvnet (1.7333s) GuidedBackprop (1.7796s) PatterNet (1.7825s) Attribution (1.7842s) LRP Epsilon (1.8008s) LRP CompositeA (4.3155s) LRP CompositeB (7.4140s) Input (0.0012s) Gradient (1.7538s) SmoothGrad (108.8910s) IntegratedGradients (108.8157s) Deconvnet (1.7431s) GuidedBackprop (1.7872s) PatterNet (1.7827s) Attribution (1.7928s) LRP Epsilon (1.7837s) LRP CompositeA (4.2446s) LRP CompositeB (7.4347s) Input (0.0012s) Gradient (1.7662s) SmoothGrad (108.5939s) IntegratedGradients (108.1687s) Deconvnet (1.7470s) GuidedBackprop (1.7544s) PatterNet (1.7695s) Attribution (1.7879s) LRP Epsilon (1.7691s) LRP CompositeA (4.2525s) LRP CompositeB (7.3925s) "
     ]
    }
   ],
   "source": [
    "# Create analyzers.\n",
    "analyzers = []\n",
    "print('Create analyzer for ', flush=True)\n",
    "for method in methods:\n",
    "    print('{} '.format(''.join(method[3])), flush=True)\n",
    "    analyzers.append(innvestigate.create_analyzer(method[0], \n",
    "                                                  model, \n",
    "                                                  **method[1]))\n",
    "\n",
    "# Apply analyzers to trained VGG model.\n",
    "color_conversion = \"BGRtoRGB\" if net[\"color_coding\"] == \"BGR\" else None\n",
    "analysis = np.zeros([len(images), len(analyzers), 224, 224, 3])\n",
    "text = []\n",
    "for i, (image, y) in enumerate(images):\n",
    "    image = image[None, :, :, :]\n",
    "    # Predict label.\n",
    "    x = imgnetutils.preprocess(image, net)\n",
    "    presm = model.predict_on_batch(x)[0]\n",
    "    prob = modelp.predict_on_batch(x)[0]\n",
    "    y_hat = prob.argmax()\n",
    "\n",
    "   # Save label, pre-softmax activation, probabilistic output and predicted label for plots\n",
    "    text.append((\"%s\" %label_to_class_name[y], \"%.2f\" %presm.max(), \n",
    "                 \"%.2f\" % prob.max(), \"%s\" %label_to_class_name[y_hat]))\n",
    "    \n",
    "    for aidx, analyzer in enumerate(analyzers):\n",
    "        # Measure execution time\n",
    "        t_start = time.time()\n",
    "        print('{} '.format(''.join(methods[aidx][-1])), end='', flush=True)\n",
    "        \n",
    "        is_input_analyzer = methods[aidx][0] == \"input\"\n",
    "        # Analyze.\n",
    "        a = analyzer.analyze(image if is_input_analyzer else x)\n",
    "        \n",
    "        t_elapsed = time.time() - t_start\n",
    "        print('({:.4f}s) '.format(t_elapsed), end='', flush=True)\n",
    "        \n",
    "        # Postprocess.\n",
    "        if not np.all(np.isfinite(a)):\n",
    "            print(\"Image %i, analysis of %s not finite: nan %s inf %s\" %\n",
    "                  (i, methods[aidx][3],\n",
    "                   np.any(np.isnan(a)), np.any(np.isinf(a))))\n",
    "        if not is_input_analyzer:\n",
    "            a = imgnetutils.postprocess(a, color_conversion, channels_first)\n",
    "        a = methods[aidx][2](a)\n",
    "        analysis[i, aidx] = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the analysis.\n",
    "\n",
    "grid = [[analysis[i, j] for j in range(analysis.shape[1])]\n",
    "        for i in range(analysis.shape[0])]  \n",
    "row_labels = text\n",
    "col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "eutils.plot_image_grid(grid, row_labels, col_labels,\n",
    "                       row_label_offset=0,\n",
    "                       col_label_offset=0,\n",
    "                       is_fontsize_adaptive=False,\n",
    "                       usetex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
